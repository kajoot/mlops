# Docker Compose configuration for MLOps services
# Services: MLflow, API v1, API v2

version: '3.8'

services:
  # MLflow Tracking Server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.18.0
    container_name: mlflow-server
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
      - ./mlartifacts:/mlflow/mlartifacts
    command: >
      mlflow server
      --backend-store-uri file:///mlflow/mlruns
      --default-artifact-root file:///mlflow/mlartifacts
      --host 0.0.0.0
      --port 5000
    networks:
      - mlops-network

  # Inference API - Version 1
  api-v1:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: iris-api-v1
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=models/logistic_regression_baseline.joblib
      - MODEL_VERSION=1.0
    volumes:
      - ./models:/app/models:ro
      - ./api:/app/api:ro
    networks:
      - mlops-network
    depends_on:
      - mlflow

  # Inference API - Version 2 (for rollback demo)
  api-v2:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: iris-api-v2
    ports:
      - "8001:8000"
    environment:
      - MODEL_PATH=models/svm_baseline.joblib
      - MODEL_VERSION=2.0
    volumes:
      - ./models:/app/models:ro
      - ./api:/app/api:ro
    networks:
      - mlops-network
    depends_on:
      - mlflow
    profiles:
      - v2

networks:
  mlops-network:
    driver: bridge

volumes:
  mlruns:
  mlartifacts:
